To run aplication run the following command: 

scala -cp .\target\project-1.0-jar-with-dependencies.jar com.company.project.App




Some of the resources researched to make this challenge:
 
https://bigdataenthusiast.medium.com/apache-spark-explode-function-f8c0ef87452
https://www.aporia.com/resources/how-to/group-dataframe-rows-into-list/
https://sparkbyexamples.com/pandas/pandas-group-dataframe-rows-list-groupby/
https://sparkbyexamples.com/pyspark/pyspark-groupby-explained-with-example/#google_vignette
https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate-explained/
https://sparkbyexamples.com/pyspark/pyspark-withcolumn/#google_vignette
https://sparkbyexamples.com/spark/convert-string-to-date-format-spark-sql/
https://medium.com/@suffyan.asad1/spark-essentials-a-guide-to-setting-up-and-running-spark-projects-with-scala-and-sbt-80e2680d3528
https://medium.com/@er.janik26/create-spark-application-from-scrach-and-understands-sparksession-397194d45c6e
https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications
https://spark.apache.org/docs/2.2.1/sql-programming-guide.html
https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html
https://kb.databricks.com/data/join-two-dataframes-duplicated-columns.html
